{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'answer', 'distractor1', 'distractor2', 'distractor(unsure)', 'label', 'choice_list', 'choice_order'],\n",
      "        num_rows: 405\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'answer', 'distractor1', 'distractor2', 'distractor(unsure)', 'label', 'choice_list', 'choice_order'],\n",
      "        num_rows: 102\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'answer', 'distractor1', 'distractor2', 'distractor(unsure)', 'label', 'choice_list', 'choice_order'],\n",
      "        num_rows: 507\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'answer', 'distractor1', 'distractor2', 'distractor(unsure)', 'label', 'choice_list', 'choice_order'],\n",
      "        num_rows: 507\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "sentence_puzzle_data = np.load('SP-train.npy',allow_pickle=True)\n",
    "\n",
    "train_ratio = 0.8\n",
    "total_indices = np.shape(sentence_puzzle_data)[0]\n",
    "# train_indices_sentence  = np.random.choice(all_indices,size = int(train_ratio*np.shape(sentence_puzzle_data)[0]),replace=False)\n",
    "# test_indices_sentence = np.setdiff1d(all_indices, train_indices_sentence)\n",
    "thresh = (int(train_ratio*(total_indices/3)))*3\n",
    "train_elements_sentence =np.array(sentence_puzzle_data)[:thresh].tolist()\n",
    "test_elements_sentence = np.array(sentence_puzzle_data)[thresh:].tolist()\n",
    "all_elements_sentence = np.array(sentence_puzzle_data).tolist()\n",
    "# print(test_elements_sentence[1])\n",
    "for item in train_elements_sentence :\n",
    "    item['distractor1'] = str(item['distractor1'])\n",
    "    item['distractor2'] = str(item['distractor2'])\n",
    "    item['distractor(unsure)'] = str(item['distractor(unsure)'])\n",
    "for item in test_elements_sentence :\n",
    "    item['distractor1'] = str(item['distractor1'])\n",
    "    item['distractor2'] = str(item['distractor2'])\n",
    "    item['distractor(unsure)'] = str(item['distractor(unsure)'])\n",
    "for item in all_elements_sentence :\n",
    "    item['distractor1'] = str(item['distractor1'])\n",
    "    item['distractor2'] = str(item['distractor2'])\n",
    "    item['distractor(unsure)'] = str(item['distractor(unsure)'])\n",
    "# print(sentence_puzzle_data)\n",
    "sentence_puzzle_data = DatasetDict({\"train\": Dataset.from_pandas(pd.DataFrame(train_elements_sentence)),\"test\": Dataset.from_pandas(pd.DataFrame(test_elements_sentence))})\n",
    "all_sentence_puzzle_data = DatasetDict({\"train\": Dataset.from_pandas(pd.DataFrame(all_elements_sentence)),\"test\": Dataset.from_pandas(pd.DataFrame(all_elements_sentence))})\n",
    "# word_puzzle_data = np.load('BrainTeaser/word_puzzle.npy',allow_pickle=True).tolist()\n",
    "\n",
    "print(sentence_puzzle_data)\n",
    "print(all_sentence_puzzle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'answer', 'distractor1', 'distractor2', 'distractor(unsure)', 'label', 'choice_list', 'choice_order'],\n",
      "        num_rows: 276\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'answer', 'distractor1', 'distractor2', 'distractor(unsure)', 'label', 'choice_list', 'choice_order'],\n",
      "        num_rows: 120\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'answer', 'distractor1', 'distractor2', 'distractor(unsure)', 'label', 'choice_list', 'choice_order'],\n",
      "        num_rows: 396\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'answer', 'distractor1', 'distractor2', 'distractor(unsure)', 'label', 'choice_list', 'choice_order'],\n",
      "        num_rows: 396\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "word_puzzle_data = np.load('WP-train.npy',allow_pickle=True)\n",
    "\n",
    "train_ratio = 0.7\n",
    "total_indices = np.shape(word_puzzle_data)[0]\n",
    "# train_indices_sentence  = np.random.choice(all_indices,size = int(train_ratio*np.shape(sentence_puzzle_data)[0]),replace=False)\n",
    "# test_indices_sentence = np.setdiff1d(all_indices, train_indices_sentence)\n",
    "thresh = (int(train_ratio*(total_indices/3)))*3\n",
    "train_elements_word =np.array(word_puzzle_data)[:thresh].tolist()\n",
    "test_elements_word = np.array(word_puzzle_data)[thresh:].tolist()\n",
    "all_elements_word = np.array(word_puzzle_data).tolist()\n",
    "# print(test_elements_sentence[1])\n",
    "for item in train_elements_word :\n",
    "    item['distractor1'] = str(item['distractor1'])\n",
    "    item['distractor2'] = str(item['distractor2'])\n",
    "    item['distractor(unsure)'] = str(item['distractor(unsure)'])\n",
    "for item in test_elements_word :\n",
    "    item['distractor1'] = str(item['distractor1'])\n",
    "    item['distractor2'] = str(item['distractor2'])\n",
    "    item['distractor(unsure)'] = str(item['distractor(unsure)'])\n",
    "for item in all_elements_word :\n",
    "    item['distractor1'] = str(item['distractor1'])\n",
    "    item['distractor2'] = str(item['distractor2'])\n",
    "    item['distractor(unsure)'] = str(item['distractor(unsure)'])\n",
    "# print(sentence_puzzle_data)\n",
    "word_puzzle_data = DatasetDict({\"train\": Dataset.from_pandas(pd.DataFrame(train_elements_word)),\"test\": Dataset.from_pandas(pd.DataFrame(test_elements_word))})\n",
    "all_word_puzzle_data = DatasetDict({\"train\": Dataset.from_pandas(pd.DataFrame(all_elements_word)),\"test\": Dataset.from_pandas(pd.DataFrame(all_elements_word))})\n",
    "# word_puzzle_data = np.load('BrainTeaser/word_puzzle.npy',allow_pickle=True).tolist()\n",
    "\n",
    "print(word_puzzle_data)\n",
    "print(all_word_puzzle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, XLMRobertaXLModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ending_names = [\"answer\", \"distrator1\", \"distrator2\", \"distrator(unsure)\"]\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 4 for context in examples[\"question\"]]\n",
    "    question_headers = examples[\"question\"]\n",
    "    # second_sentences = [[f\"{examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)]\n",
    "    # print(examples[\"choice_list\"][0][1])\n",
    "    second_sentences = [[f\"{examples['choice_list'][0][ind]}\" for ind in np.arange(4)] for i, header in enumerate(question_headers)]\n",
    "\n",
    "\n",
    "    # print(second_sentences)\n",
    "\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "    # print(second_sentences)\n",
    "\n",
    "    tokenized_examples = tokenizer( second_sentences, first_sentences, truncation=True)\n",
    "    # tokenized_examples = tokenizer(  first_sentences, second_sentences, truncation=True)\n",
    "    return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = preprocess_function(sentence_puzzle_data[\"train\"][0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['SP-0'],\n",
       " 'question': ['Mr. and Mrs. Mustard have six daughters and each daughter has one brother. But there are only 9 people in the family, how is that possible?'],\n",
       " 'answer': ['Each daughter shares the same brother.'],\n",
       " 'distractor1': ['Some daughters get married and have their own family.'],\n",
       " 'distractor2': ['Some brothers were not loved by family and moved away.'],\n",
       " 'distractor(unsure)': ['None of above.'],\n",
       " 'label': [1],\n",
       " 'choice_list': [['Some daughters get married and have their own family.',\n",
       "   'Each daughter shares the same brother.',\n",
       "   'Some brothers were not loved by family and moved away.',\n",
       "   'None of above.']],\n",
       " 'choice_order': [[1, 0, 2, 3]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_puzzle_data[\"train\"][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 405/405 [00:00<00:00, 9334.09 examples/s]\n",
      "Map: 100%|██████████| 102/102 [00:00<00:00, 9484.33 examples/s]\n",
      "Map: 100%|██████████| 507/507 [00:00<00:00, 9721.78 examples/s]\n",
      "Map: 100%|██████████| 507/507 [00:00<00:00, 4534.55 examples/s]\n",
      "Map: 100%|██████████| 276/276 [00:00<00:00, 18969.42 examples/s]\n",
      "Map: 100%|██████████| 120/120 [00:00<00:00, 14859.81 examples/s]\n",
      "Map: 100%|██████████| 396/396 [00:00<00:00, 20683.47 examples/s]\n",
      "Map: 100%|██████████| 396/396 [00:00<00:00, 18400.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence_puzzle_data = sentence_puzzle_data.map(preprocess_function, batched=True)\n",
    "all_tokenized_sentence_puzzle_data = all_sentence_puzzle_data.map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_word_puzzle_data = word_puzzle_data.map(preprocess_function, batched=True)\n",
    "all_tokenized_word_puzzle_data = all_word_puzzle_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = predictions == labels\n",
    "    sz = acc.shape[0]\n",
    "    original = (np.sum(acc[0::3])*3)/sz\n",
    "    semantic = (np.sum(acc[1::3])*3)/sz\n",
    "    context = (np.sum(acc[2::3])*3)/sz\n",
    "    average = np.sum(acc)/sz\n",
    "    orisem = 0\n",
    "    orisemcon = 0\n",
    "    for id in range(0, sz, 3):\n",
    "        orisem += (acc[id] and acc[id+1])\n",
    "        orisemcon += (acc[id] and acc[id+1] and acc[id+2])\n",
    "    orisem = (orisem*3)/sz\n",
    "    orisemcon = (orisemcon*3)/sz\n",
    "    return {'average': average,'original': original, 'semantic': semantic, 'context': context, 'orisem': orisem, 'orisemcon': orisemcon}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "\n",
    "sentence_model = AutoModelForMultipleChoice.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_swag_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub = False,\n",
    ")\n",
    "\n",
    "all_sentence_trainer = Trainer(\n",
    "    model=sentence_model,\n",
    "    args=training_args,\n",
    "    train_dataset=all_tokenized_sentence_puzzle_data[\"train\"],\n",
    "    eval_dataset=all_tokenized_sentence_puzzle_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "sentence_trainer = Trainer(\n",
    "    model=sentence_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_sentence_puzzle_data[\"train\"],\n",
    "    eval_dataset=tokenized_sentence_puzzle_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 8/16 [00:31<00:31,  3.89s/it]"
     ]
    }
   ],
   "source": [
    "all_sentence_trainer.evaluate() # ALL DATASET ZERO SHOT SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  6.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.3876346349716187,\n",
       " 'eval_average': 0.22549019607843138,\n",
       " 'eval_original': 0.17647058823529413,\n",
       " 'eval_semantic': 0.29411764705882354,\n",
       " 'eval_context': 0.20588235294117646,\n",
       " 'eval_orisem': 0.08823529411764706,\n",
       " 'eval_orisemcon': 0.029411764705882353,\n",
       " 'eval_runtime': 0.838,\n",
       " 'eval_samples_per_second': 121.714,\n",
       " 'eval_steps_per_second': 4.773}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_trainer.evaluate() # VAL DATASET ZERO SHOT SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 10%|█         | 13/130 [00:08<01:15,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.404394268989563, 'eval_average': 0.20588235294117646, 'eval_original': 0.17647058823529413, 'eval_semantic': 0.14705882352941177, 'eval_context': 0.29411764705882354, 'eval_orisem': 0.058823529411764705, 'eval_orisemcon': 0.058823529411764705, 'eval_runtime': 0.8514, 'eval_samples_per_second': 119.809, 'eval_steps_per_second': 4.698, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "                                                \n",
      " 20%|██        | 26/130 [00:17<00:56,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3836140632629395, 'eval_average': 0.3137254901960784, 'eval_original': 0.2647058823529412, 'eval_semantic': 0.35294117647058826, 'eval_context': 0.3235294117647059, 'eval_orisem': 0.11764705882352941, 'eval_orisemcon': 0.029411764705882353, 'eval_runtime': 0.8479, 'eval_samples_per_second': 120.297, 'eval_steps_per_second': 4.718, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "                                                \n",
      " 30%|███       | 39/130 [00:26<00:47,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3841089010238647, 'eval_average': 0.30392156862745096, 'eval_original': 0.38235294117647056, 'eval_semantic': 0.3235294117647059, 'eval_context': 0.20588235294117646, 'eval_orisem': 0.11764705882352941, 'eval_orisemcon': 0.0, 'eval_runtime': 0.8518, 'eval_samples_per_second': 119.751, 'eval_steps_per_second': 4.696, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "                                                \n",
      " 40%|████      | 52/130 [00:35<00:40,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3849141597747803, 'eval_average': 0.30392156862745096, 'eval_original': 0.38235294117647056, 'eval_semantic': 0.2647058823529412, 'eval_context': 0.2647058823529412, 'eval_orisem': 0.058823529411764705, 'eval_orisemcon': 0.0, 'eval_runtime': 0.8517, 'eval_samples_per_second': 119.765, 'eval_steps_per_second': 4.697, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "                                                \n",
      " 50%|█████     | 65/130 [00:44<00:34,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3867459297180176, 'eval_average': 0.24509803921568626, 'eval_original': 0.23529411764705882, 'eval_semantic': 0.17647058823529413, 'eval_context': 0.3235294117647059, 'eval_orisem': 0.029411764705882353, 'eval_orisemcon': 0.029411764705882353, 'eval_runtime': 0.8505, 'eval_samples_per_second': 119.928, 'eval_steps_per_second': 4.703, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "                                                \n",
      " 60%|██████    | 78/130 [00:53<00:27,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3871958255767822, 'eval_average': 0.2647058823529412, 'eval_original': 0.2647058823529412, 'eval_semantic': 0.23529411764705882, 'eval_context': 0.29411764705882354, 'eval_orisem': 0.029411764705882353, 'eval_orisemcon': 0.0, 'eval_runtime': 0.8528, 'eval_samples_per_second': 119.599, 'eval_steps_per_second': 4.69, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "                                                \n",
      " 70%|███████   | 91/130 [01:02<00:21,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3870545625686646, 'eval_average': 0.22549019607843138, 'eval_original': 0.23529411764705882, 'eval_semantic': 0.23529411764705882, 'eval_context': 0.20588235294117646, 'eval_orisem': 0.029411764705882353, 'eval_orisemcon': 0.0, 'eval_runtime': 0.8509, 'eval_samples_per_second': 119.878, 'eval_steps_per_second': 4.701, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "                                                 \n",
      " 80%|████████  | 104/130 [01:12<00:14,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3879913091659546, 'eval_average': 0.20588235294117646, 'eval_original': 0.20588235294117646, 'eval_semantic': 0.2647058823529412, 'eval_context': 0.14705882352941177, 'eval_orisem': 0.029411764705882353, 'eval_orisemcon': 0.0, 'eval_runtime': 0.8498, 'eval_samples_per_second': 120.025, 'eval_steps_per_second': 4.707, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "                                                 \n",
      " 90%|█████████ | 117/130 [01:21<00:06,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3887790441513062, 'eval_average': 0.23529411764705882, 'eval_original': 0.2647058823529412, 'eval_semantic': 0.23529411764705882, 'eval_context': 0.20588235294117646, 'eval_orisem': 0.029411764705882353, 'eval_orisemcon': 0.0, 'eval_runtime': 0.8497, 'eval_samples_per_second': 120.047, 'eval_steps_per_second': 4.708, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "                                                 \n",
      "100%|██████████| 130/130 [01:30<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3890349864959717, 'eval_average': 0.20588235294117646, 'eval_original': 0.23529411764705882, 'eval_semantic': 0.23529411764705882, 'eval_context': 0.14705882352941177, 'eval_orisem': 0.029411764705882353, 'eval_orisemcon': 0.0, 'eval_runtime': 0.8498, 'eval_samples_per_second': 120.034, 'eval_steps_per_second': 4.707, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:31<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 91.5351, 'train_samples_per_second': 44.245, 'train_steps_per_second': 1.42, 'train_loss': 1.3870843740609975, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=130, training_loss=1.3870843740609975, metrics={'train_runtime': 91.5351, 'train_samples_per_second': 44.245, 'train_steps_per_second': 1.42, 'train_loss': 1.3870843740609975, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_trainer.train() # VAL DATASET FEW SHOT SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of YosoForMultipleChoice were not initialized from the model checkpoint at uw-madison/yoso-4096 and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "word_model = AutoModelForMultipleChoice.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_word_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub = False,\n",
    ")\n",
    "\n",
    "all_word_trainer = Trainer(\n",
    "    model=word_model,\n",
    "    args=training_args,\n",
    "    train_dataset=all_tokenized_word_puzzle_data[\"train\"],\n",
    "    eval_dataset=all_tokenized_word_puzzle_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "word_trainer = Trainer(\n",
    "    model=word_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_word_puzzle_data[\"train\"],\n",
    "    eval_dataset=tokenized_word_puzzle_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|██████████| 13/13 [00:02<00:00,  5.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.386165738105774,\n",
       " 'eval_average': 0.2702020202020202,\n",
       " 'eval_original': 0.2196969696969697,\n",
       " 'eval_semantic': 0.29545454545454547,\n",
       " 'eval_context': 0.29545454545454547,\n",
       " 'eval_orisem': 0.06060606060606061,\n",
       " 'eval_orisemcon': 0.015151515151515152,\n",
       " 'eval_runtime': 2.3892,\n",
       " 'eval_samples_per_second': 165.744,\n",
       " 'eval_steps_per_second': 5.441}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_word_trainer.evaluate() # ALL DATASET ZERO SHOT WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  7.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.387241005897522,\n",
       " 'eval_average': 0.175,\n",
       " 'eval_original': 0.225,\n",
       " 'eval_semantic': 0.2,\n",
       " 'eval_context': 0.1,\n",
       " 'eval_orisem': 0.05,\n",
       " 'eval_orisemcon': 0.0,\n",
       " 'eval_runtime': 0.6882,\n",
       " 'eval_samples_per_second': 174.373,\n",
       " 'eval_steps_per_second': 5.812}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_trainer.evaluate() # VAL DATASET ZERO SHOT WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 9/180 [00:03<01:12,  2.37it/s]\n",
      "  5%|▌         | 9/180 [00:04<01:12,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3867590427398682, 'eval_average': 0.2, 'eval_original': 0.15, 'eval_semantic': 0.2, 'eval_context': 0.25, 'eval_orisem': 0.05, 'eval_orisemcon': 0.0, 'eval_runtime': 0.7036, 'eval_samples_per_second': 170.554, 'eval_steps_per_second': 5.685, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 10%|█         | 18/180 [00:09<01:14,  2.17it/s]\n",
      " 10%|█         | 18/180 [00:10<01:14,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.386548399925232, 'eval_average': 0.25833333333333336, 'eval_original': 0.3, 'eval_semantic': 0.25, 'eval_context': 0.225, 'eval_orisem': 0.125, 'eval_orisemcon': 0.0, 'eval_runtime': 0.7042, 'eval_samples_per_second': 170.395, 'eval_steps_per_second': 5.68, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 15%|█▌        | 27/180 [00:15<01:09,  2.19it/s]\n",
      " 15%|█▌        | 27/180 [00:15<01:09,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.381896734237671, 'eval_average': 0.3, 'eval_original': 0.225, 'eval_semantic': 0.35, 'eval_context': 0.325, 'eval_orisem': 0.05, 'eval_orisemcon': 0.0, 'eval_runtime': 0.7045, 'eval_samples_per_second': 170.336, 'eval_steps_per_second': 5.678, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 20%|██        | 36/180 [00:20<01:05,  2.18it/s]\n",
      " 20%|██        | 36/180 [00:21<01:05,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3948993682861328, 'eval_average': 0.16666666666666666, 'eval_original': 0.15, 'eval_semantic': 0.2, 'eval_context': 0.15, 'eval_orisem': 0.025, 'eval_orisemcon': 0.0, 'eval_runtime': 0.7046, 'eval_samples_per_second': 170.314, 'eval_steps_per_second': 5.677, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 25%|██▌       | 45/180 [00:26<01:01,  2.21it/s]\n",
      " 25%|██▌       | 45/180 [00:27<01:01,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3937214612960815, 'eval_average': 0.18333333333333332, 'eval_original': 0.25, 'eval_semantic': 0.125, 'eval_context': 0.175, 'eval_orisem': 0.05, 'eval_orisemcon': 0.0, 'eval_runtime': 0.7047, 'eval_samples_per_second': 170.276, 'eval_steps_per_second': 5.676, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 30%|███       | 54/180 [00:32<00:56,  2.22it/s]\n",
      " 30%|███       | 54/180 [00:32<00:56,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.402025818824768, 'eval_average': 0.125, 'eval_original': 0.125, 'eval_semantic': 0.15, 'eval_context': 0.1, 'eval_orisem': 0.025, 'eval_orisemcon': 0.0, 'eval_runtime': 0.7038, 'eval_samples_per_second': 170.492, 'eval_steps_per_second': 5.683, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 35%|███▌      | 63/180 [00:37<00:53,  2.18it/s]\n",
      " 35%|███▌      | 63/180 [00:38<00:53,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3875696659088135, 'eval_average': 0.2916666666666667, 'eval_original': 0.375, 'eval_semantic': 0.225, 'eval_context': 0.275, 'eval_orisem': 0.1, 'eval_orisemcon': 0.075, 'eval_runtime': 0.7036, 'eval_samples_per_second': 170.557, 'eval_steps_per_second': 5.685, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 40%|████      | 72/180 [00:43<00:48,  2.21it/s]\n",
      " 40%|████      | 72/180 [00:44<00:48,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3864659070968628, 'eval_average': 0.3, 'eval_original': 0.35, 'eval_semantic': 0.275, 'eval_context': 0.275, 'eval_orisem': 0.05, 'eval_orisemcon': 0.0, 'eval_runtime': 0.7027, 'eval_samples_per_second': 170.77, 'eval_steps_per_second': 5.692, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 45%|████▌     | 81/180 [00:49<00:45,  2.17it/s]\n",
      " 45%|████▌     | 81/180 [00:49<00:45,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3911716938018799, 'eval_average': 0.15833333333333333, 'eval_original': 0.2, 'eval_semantic': 0.075, 'eval_context': 0.2, 'eval_orisem': 0.025, 'eval_orisemcon': 0.025, 'eval_runtime': 0.7026, 'eval_samples_per_second': 170.792, 'eval_steps_per_second': 5.693, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 50%|█████     | 90/180 [00:54<00:40,  2.21it/s]\n",
      " 50%|█████     | 90/180 [00:55<00:40,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3870855569839478, 'eval_average': 0.23333333333333334, 'eval_original': 0.175, 'eval_semantic': 0.275, 'eval_context': 0.25, 'eval_orisem': 0.05, 'eval_orisemcon': 0.0, 'eval_runtime': 0.7053, 'eval_samples_per_second': 170.141, 'eval_steps_per_second': 5.671, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 55%|█████▌    | 99/180 [01:00<00:37,  2.19it/s]\n",
      " 55%|█████▌    | 99/180 [01:01<00:37,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3833848237991333, 'eval_average': 0.26666666666666666, 'eval_original': 0.325, 'eval_semantic': 0.225, 'eval_context': 0.25, 'eval_orisem': 0.075, 'eval_orisemcon': 0.05, 'eval_runtime': 0.7063, 'eval_samples_per_second': 169.908, 'eval_steps_per_second': 5.664, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 60%|██████    | 108/180 [01:06<00:32,  2.20it/s]\n",
      " 60%|██████    | 108/180 [01:06<00:32,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.383020281791687, 'eval_average': 0.24166666666666667, 'eval_original': 0.25, 'eval_semantic': 0.2, 'eval_context': 0.275, 'eval_orisem': 0.05, 'eval_orisemcon': 0.0, 'eval_runtime': 0.7043, 'eval_samples_per_second': 170.394, 'eval_steps_per_second': 5.68, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 65%|██████▌   | 117/180 [01:11<00:28,  2.21it/s]\n",
      " 65%|██████▌   | 117/180 [01:12<00:28,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3836122751235962, 'eval_average': 0.325, 'eval_original': 0.4, 'eval_semantic': 0.275, 'eval_context': 0.3, 'eval_orisem': 0.15, 'eval_orisemcon': 0.075, 'eval_runtime': 0.7025, 'eval_samples_per_second': 170.827, 'eval_steps_per_second': 5.694, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 70%|███████   | 126/180 [01:17<00:24,  2.17it/s]\n",
      " 70%|███████   | 126/180 [01:18<00:24,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3841021060943604, 'eval_average': 0.2916666666666667, 'eval_original': 0.325, 'eval_semantic': 0.275, 'eval_context': 0.275, 'eval_orisem': 0.125, 'eval_orisemcon': 0.05, 'eval_runtime': 0.7041, 'eval_samples_per_second': 170.426, 'eval_steps_per_second': 5.681, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 75%|███████▌  | 135/180 [01:23<00:20,  2.18it/s]\n",
      " 75%|███████▌  | 135/180 [01:23<00:20,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3851391077041626, 'eval_average': 0.18333333333333332, 'eval_original': 0.175, 'eval_semantic': 0.225, 'eval_context': 0.15, 'eval_orisem': 0.05, 'eval_orisemcon': 0.0, 'eval_runtime': 0.7041, 'eval_samples_per_second': 170.431, 'eval_steps_per_second': 5.681, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 80%|████████  | 144/180 [01:28<00:16,  2.19it/s]\n",
      " 80%|████████  | 144/180 [01:29<00:16,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3860571384429932, 'eval_average': 0.15, 'eval_original': 0.125, 'eval_semantic': 0.15, 'eval_context': 0.175, 'eval_orisem': 0.025, 'eval_orisemcon': 0.025, 'eval_runtime': 0.7055, 'eval_samples_per_second': 170.085, 'eval_steps_per_second': 5.67, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 85%|████████▌ | 153/180 [01:34<00:12,  2.18it/s]\n",
      " 85%|████████▌ | 153/180 [01:35<00:12,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3880212306976318, 'eval_average': 0.15, 'eval_original': 0.1, 'eval_semantic': 0.2, 'eval_context': 0.15, 'eval_orisem': 0.025, 'eval_orisemcon': 0.025, 'eval_runtime': 0.7052, 'eval_samples_per_second': 170.155, 'eval_steps_per_second': 5.672, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 90%|█████████ | 162/180 [01:40<00:08,  2.18it/s]\n",
      " 90%|█████████ | 162/180 [01:41<00:08,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3889636993408203, 'eval_average': 0.175, 'eval_original': 0.1, 'eval_semantic': 0.25, 'eval_context': 0.175, 'eval_orisem': 0.025, 'eval_orisemcon': 0.025, 'eval_runtime': 0.7043, 'eval_samples_per_second': 170.375, 'eval_steps_per_second': 5.679, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      " 95%|█████████▌| 171/180 [01:46<00:04,  2.14it/s]\n",
      " 95%|█████████▌| 171/180 [01:46<00:04,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3885098695755005, 'eval_average': 0.16666666666666666, 'eval_original': 0.125, 'eval_semantic': 0.225, 'eval_context': 0.15, 'eval_orisem': 0.025, 'eval_orisemcon': 0.025, 'eval_runtime': 0.7063, 'eval_samples_per_second': 169.888, 'eval_steps_per_second': 5.663, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tushargoyal/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|██████████| 180/180 [01:51<00:00,  2.17it/s]\n",
      "100%|██████████| 180/180 [01:52<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3884633779525757, 'eval_average': 0.16666666666666666, 'eval_original': 0.125, 'eval_semantic': 0.225, 'eval_context': 0.15, 'eval_orisem': 0.025, 'eval_orisemcon': 0.025, 'eval_runtime': 0.7029, 'eval_samples_per_second': 170.73, 'eval_steps_per_second': 5.691, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [01:53<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 113.5745, 'train_samples_per_second': 48.602, 'train_steps_per_second': 1.585, 'train_loss': 1.3864696078830294, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=180, training_loss=1.3864696078830294, metrics={'train_runtime': 113.5745, 'train_samples_per_second': 48.602, 'train_steps_per_second': 1.585, 'train_loss': 1.3864696078830294, 'epoch': 20.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_trainer.train() # VAL DATASET FEW SHOT WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_trainer.evaluate() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
